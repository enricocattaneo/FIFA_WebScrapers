{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scrape Players Data (from fifaindex.com)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE**: Using the BeautifulSoup package, scrape football teams' information about fifa attributes from the fifaindex.com website (only five major european leagues). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from random import uniform\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "from random import choice\n",
    "from urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import urllib3.exceptions\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is necessary to make calls using different headers respecting also a sleep time between each call (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(pageURL):\n",
    "    \"\"\"Given a URL, makes requests using different headers and with a sleep time (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. \n",
    "    Parameters:\n",
    "    - pageURL: web page's URL\"\"\"\n",
    "    global errors\n",
    "    headers_list = [\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "           \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "           \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "           \"Referer\": \"https://www.google.com/\",\n",
    "           \"DNT\": \"1\",\n",
    "           \"Connection\": \"keep-alive\",\n",
    "           \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"none\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"same-origin\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-User\": \"?1\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "           }]\n",
    "           \n",
    "    time.sleep(uniform(0, 2.25))\n",
    "    req = requests.Session()\n",
    "    headers = choice(headers_list)\n",
    "    retry = Retry(total=8, connect=8, backoff_factor=2)\n",
    "    req.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "     \n",
    "    try:\n",
    "        html = req.get(pageURL, headers=headers, timeout=12)\n",
    "    except requests.packages.urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('requests.packages.urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.ConnectionError:\n",
    "        errors.append('requests.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.ConnectionError:\n",
    "        errors.append('urllib3.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        errors.append('requests.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except Exception as ex:\n",
    "        errors.append(str(ex))\n",
    "        html = requests.get('http://google.com/f')\n",
    "          \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of the seasons and leagues to consider, this function just create the URL of the fifaindex.com specific webpage to scrape from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allpages(seasons, championship):\n",
    "    \"\"\"Given a list of the seasons and leagues to consider, this function just creates the URL for fifaindex.com specific webpages (linked to the desired season and league).\n",
    "    Parameters:\n",
    "    - seasons: list formatted considering fifaindex.com season type\n",
    "    - championship: list formatted considering fifaindex.com league type\"\"\"    \n",
    "    global all_pages\n",
    "    for j in seasons:\n",
    "        for k in championship:\n",
    "            Newpage_to_add = 'https://www.fifaindex.com/teams/' + j + '/?' + k + '&order=desc'\n",
    "               \n",
    "            all_pages.add(Newpage_to_add)\n",
    "    return all_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Teams' Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a webpage's URL, this function returns all teams' URLs included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teamlinks(page_url): \n",
    "    \"\"\"Given a webpage's URL containing teams' links, returns all teams' URLs included.\n",
    "    Parameters:\n",
    "    - page_url: page URL\n",
    "    \"\"\"  \n",
    "    global teams_links\n",
    "    right_ending = re.findall('\\/fifa.*\\/', page_url)[0]\n",
    "    html = make_request(pageURL = page_url)\n",
    "    bs = BeautifulSoup(html.text, 'lxml')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/team/)')): \n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in teams_links: \n",
    "                teams_links.add(re.sub('\\/fifa.*\\/$', right_ending, link.attrs['href'])) \n",
    "    return teams_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Team's Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a specific team webpage, this function returns the desired information using the BeautifulSoup package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team(website_page):\n",
    "    \"\"\"Given a specific team fifaindex.com profile, returns the desired information using the BeautifulSoup package.\n",
    "    Parameters:\n",
    "    - website_page: team profile on fifaindex.com\n",
    "    \"\"\"   \n",
    "    # Request\n",
    "    team_html = make_request(pageURL = website_page)\n",
    "    soup = BeautifulSoup(team_html.text, 'lxml')\n",
    "    # Team information \n",
    "    dd = {}\n",
    "    dd['TeamName'] = (re.sub(' FIFA.+', '', soup.h1.text).strip()) # team name\n",
    "    # Observation Date\n",
    "    for date in soup.select('.dropdown:nth-child(3) .dropdown-toggle'):\n",
    "        dd['ObservationDate'] = date.text.strip()\n",
    "    # Rival team\n",
    "    if soup.find('a', class_='link-team').text is not None:\n",
    "        dd['RivalTeam'] = soup.find('a', class_='link-team').text\n",
    "    # Team attributes\n",
    "    for i in range(2,5): \n",
    "        for info in soup.select('.list-group-item:nth-child(' + str(i) + ')'):\n",
    "            match = re.match(\"(\\D+)(\\d{1,2})\", info.text, re.I)\n",
    "            dd[match.group(1)] = match.group(2)\n",
    "    # Transfer budget\n",
    "    tranfer_budget = soup.find('span',class_ = 'data-currency-euro').text \n",
    "    dd['TransferBudget'] = int(re.sub(r'[.â‚¬]', '', tranfer_budget))\n",
    "    # Team Attributes + Players Roles \n",
    "    list1, list2 = [], []\n",
    "    for value in soup.select('.card-body p'):\n",
    "        match = re.sub('\\d', '', value.text)\n",
    "        list1.append(match.strip())\n",
    "    for i in range(1,3): # exception in 2010-2018 period\n",
    "        if list1[i] == 'Passing':\n",
    "            list1[i] = 'BuildupPassing'\n",
    "    if list1[1] == 'Width': # exception in 2019- period\n",
    "        list1[1] = 'DefensiveWidth'\n",
    "    for vv in soup.select('.card-body .float-right'):\n",
    "        list2.append(vv.text)\n",
    "    for i in range(len(list1)):\n",
    "        if re.search(list2[i], list1[i]):\n",
    "            dd[re.sub(list2[i], '', list1[i]).strip()] = list2[i]\n",
    "        else:\n",
    "            dd[list1[i]] = list2[i]\n",
    "    # Team Players + Loaned Players\n",
    "    players, loaned_players = [], []\n",
    "    for player in soup.select('td:nth-child(6) .link-player'):\n",
    "        players.append(player.text)\n",
    "    dd['TeamRoster'] = players\n",
    "    for loanedplayer in soup.select('td:nth-child(4) .link-player'):\n",
    "        loaned_players.append(loanedplayer.text)\n",
    "    dd['LoanedPlayers'] = loaned_players\n",
    "\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_Create_file(path, seas, champ):\n",
    "    \"\"\" Given the season and leagues formatted lists, return an object with all teams profile links.\n",
    "    Parameters:\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\n",
    "    \"\"\"       \n",
    "    global all_pages\n",
    "    if os.path.isfile(path):\n",
    "        j_file = open(path)\n",
    "        all_teams_pages = json.load(j_file)\n",
    "        j_file.close()\n",
    "    else: \n",
    "        get_allpages(seasons=seas, championship=champ)\n",
    "        for page in tqdm_notebook(all_pages, total=len(all_pages)):\n",
    "            all_teams_pages = get_teamlinks(page)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(list(all_teams_pages), f, ensure_ascii=False, indent=4)\n",
    "        f.close()\n",
    "    return all_teams_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_list(ALL_team_player_pages):\n",
    "    \"\"\"Given all teams profile links, returns final object with info on every team.\n",
    "    Parameters:\n",
    "    - ALL_team_player_pages: every team profile link\n",
    "    \"\"\"  \n",
    "    final_list = []\n",
    "    count = 0\n",
    "    for team_page in tqdm_notebook(ALL_team_player_pages, total=len(ALL_team_player_pages)): \n",
    "        try:\n",
    "            final_list.append(scrape_team('https://www.fifaindex.com' + team_page))\n",
    "        except ValueError:\n",
    "            count += 1\n",
    "            final_list.append({'ERROR': team_page})  \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dataframe(path, seas, champ):\n",
    "    \"\"\"Given the season and leagues formatted lists, returns a Dataframe with all teams infos.\n",
    "    Parameters (same as Open_Create_file()):\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\"\"\"\n",
    "    all_teams_pages = Open_Create_file(path, seas, champ)\n",
    "    final = get_final_list(all_teams_pages)\n",
    "    if len(all_teams_pages) == len(final):\n",
    "        print('No Error')\n",
    "    else: \n",
    "        print('!!!Error!!!')\n",
    "    df = pd.DataFrame(final)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section all the previously presented functions are used to obtain a csv file with all teams attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seasons and Leagues lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "all_pages = set()\n",
    "teams_links = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Seasons and Leagues (TO CUSTOMIZED!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEAGUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Leagues\n",
    "leagues = ['league=31', 'league=13','league=16','league=19','league=53']\n",
    "# Minor Leagues\n",
    "minor_leagues = ['league=14', 'league=17', 'league=20', 'league=32', 'league=54']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ANNUAL DATA \n",
    "years = ['fifa10_6', 'fifa11_7','fifa12_8','fifa13_11','fifa14_12','fifa15_16','fifa16_19','fifa17_75','fifa18_175','fifa19_280','fifa20_358','fifa21_421','fifa22_487'] # If possible always 1st observation in September when the market is closed else August\n",
    "\n",
    "# FOR WEEKLY DATA \n",
    "weeks = {'fifa10_6', 'fifa11_7', 'fifa12_8', 'fifa12_9', 'fifa13_11', 'fifa14_12', 'fifa15_16'}\n",
    "for i in range(19,60): # FIFA16\n",
    "    weeks.add('fifa16_' + str(i))\n",
    "for j in range(74,144): # FIFA17\n",
    "    weeks.add('fifa17_' + str(j))\n",
    "for k in range(174, 246): # FIFA18\n",
    "    weeks.add('fifa18_' + str(k))\n",
    "for a in range(279, 344): # FIFA19\n",
    "    weeks.add('fifa19_' + str(a))\n",
    "for b in range(354,416): # FIFA20\n",
    "    weeks.add('fifa20_' + str(b))\n",
    "for c in range(420, 465): # FIFA21\n",
    "    weeks.add('fifa21_' + str(c))\n",
    "for d in range(487, 528): # FIFA22\n",
    "    weeks.add('fifa22_' + str(d))\n",
    "\n",
    "# FOR UPDATE: just add new weeks\n",
    "new_weeks = ['fifa15_17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RUN TO GET DATAFRAME!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_dataframe(path = 'proof.json', seas=new_weeks, champ=minor_leagues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"teams.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "73df3d2a648ddfe6e132dd0b2981f8c5ee01eb57f65aaa52301d101a94b0ebb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
