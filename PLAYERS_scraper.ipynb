{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scrape Players Data (from fifaindex.com)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE**: Using the BeautifulSoup package, scrape football players' information about fifa attributes from the fifaindex.com website (only five major european leagues). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from random import uniform\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "from random import choice\n",
    "from urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import urllib3.exceptions\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is necessary to make calls using different headers respecting also a sleep time between each call (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(pageURL):\n",
    "    \"\"\"Given a URL, makes requests using different headers and with a sleep time (good practice in web scraping applications). The function also handles some of the possible exceptions that might be risen. \n",
    "    Parameters:\n",
    "    - pageURL: web page's URL\"\"\"\n",
    "    global errors\n",
    "    headers_list = [\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "           \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "           \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "           \"Referer\": \"https://www.google.com/\",\n",
    "           \"DNT\": \"1\",\n",
    "           \"Connection\": \"keep-alive\",\n",
    "           \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"DNT\": \"1\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"none\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "           },\n",
    "          {\"Connection\": \"keep-alive\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\",\n",
    "               \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "               \"Sec-Fetch-Site\": \"same-origin\",\n",
    "               \"Sec-Fetch-Mode\": \"navigate\",\n",
    "               \"Sec-Fetch-User\": \"?1\",\n",
    "               \"Sec-Fetch-Dest\": \"document\",\n",
    "               \"Referer\": \"https://www.google.com/\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "           }]\n",
    "    time.sleep(uniform(0, 2))\n",
    "    req = requests.Session()\n",
    "    headers = choice(headers_list)\n",
    "    retry = Retry(total=8, connect=8, backoff_factor=2)\n",
    "    req.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "     \n",
    "     \n",
    "    try:\n",
    "        html = req.get(pageURL, headers=headers, timeout=12)\n",
    "    except requests.packages.urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('requests.packages.urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.MaxRetryError:\n",
    "        errors.append('urllib3.exceptions.MaxRetryError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.ConnectionError:\n",
    "        errors.append('requests.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except urllib3.exceptions.ConnectionError:\n",
    "        errors.append('urllib3.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        errors.append('requests.exceptions.ConnectionError')\n",
    "        html = requests.get('http://google.com/f')\n",
    "    except Exception as ex:\n",
    "        errors.append(str(ex))\n",
    "        html = requests.get('http://google.com/f')\n",
    "          \n",
    "     \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of the seasons and leagues to consider, this function just create the URL of the fifaindex.com specific webpage to scrape from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allpages(seasons, championship):\n",
    "    \"\"\"Given a list of the seasons and leagues to consider, this function just creates the URL for fifaindex.com specific webpages (linked to the desired season and league).\n",
    "    Parameters:\n",
    "    - seasons: list formatted considering fifaindex.com season type\n",
    "    - championship: list formatted considering fifaindex.com league type\n",
    "    \"\"\"\n",
    "    global all_pages\n",
    "    for j in seasons:\n",
    "        for k in championship:\n",
    "            for i in range(1,31):\n",
    "                if re.match('^fifa(10|11|12|13|14|15).*$', j):\n",
    "                    Newpage_to_add = 'https://www.fifaindex.com/players/' + j + '/?page=' + str(i) + '&' + k + '&order=desc'\n",
    "                else: \n",
    "                    Newpage_to_add = 'https://www.fifaindex.com/players/' + j + '/?page=' + str(i) + '&gender=0&' + k + '&order=desc' \n",
    "                all_pages.add(Newpage_to_add)\n",
    "    return all_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Players' Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a webpage's URL, this function returns all players' URLs included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playerlinks(page_url): \n",
    "    \"\"\"Given a webpage's URL containing players' links, returns all players' URLs included.\n",
    "    Parameters:\n",
    "    - page_url: page URL\n",
    "    \"\"\"   \n",
    "    global player_links\n",
    "    right_ending = re.findall('\\/fifa.*\\/', page_url)[0]\n",
    "    html = make_request(pageURL = page_url)\n",
    "    bs = BeautifulSoup(html.text, 'lxml')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/player/)')): \n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in player_links: \n",
    "                player_links.add(re.sub('\\/fifa.*\\/$', right_ending, link.attrs['href'])) \n",
    "    return player_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Players' Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a specific player webpage, this function returns the desired information using the BeautifulSoup package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player(website_page):\n",
    "    \"\"\"Given a specific player fifaindex.com profile, returns the desired information using the BeautifulSoup package.\n",
    "    Parameters:\n",
    "    - website_page: player profile on fifaindex.com\n",
    "    \"\"\"   \n",
    "    # Request\n",
    "    player_html = make_request(pageURL = website_page)\n",
    "    soup = BeautifulSoup(player_html.text, 'lxml')\n",
    "    # Attributes\n",
    "    box1 = soup.find_all('p', class_ = '')\n",
    "    ll = []\n",
    "    for val in box1:\n",
    "        ll.append(val.text.strip())\n",
    "    r = re.compile('(.+ \\d{1,2}$|.+ .+ \\d{1,2}$)')\n",
    "    newlist = list(filter(r.match, ll)) \n",
    "    newlist = [x.replace(' ', '') for x in newlist]\n",
    "    yg = dict(re.findall('^\\D+|\\d+$', x) for x in newlist)\n",
    "    # Name\n",
    "    yg['Name'] = re.sub(' FIFA.+', '', soup.h1.text).strip() # improvable\n",
    "    # Observation Date\n",
    "    for date in soup.select('.dropdown:nth-child(3) .dropdown-toggle'):\n",
    "        yg['ObservationDate'] = date.text.strip()\n",
    "    # Teams\n",
    "    for index, team in enumerate(soup.select('.link-team:nth-child(2)')):\n",
    "        if index == 0:\n",
    "            yg['Club']= team.text.strip()\n",
    "        else: \n",
    "            yg['NationalTeam']= team.text.strip()\n",
    "    # Favorite Position\n",
    "    for index, position in enumerate(soup.select('.pt-3 .position')):\n",
    "        if index == 0:\n",
    "            yg['FavoritePosition']= position.text.strip()\n",
    "    # Value and Wage\n",
    "    eur = soup.find_all('p',class_ = 'data-currency-euro')\n",
    "    for e in eur:\n",
    "        yg[e.text.split()[0]] = int(re.sub(r'[.â‚¬]', '', e.text.split()[1]))\n",
    "    # Body Measurements\n",
    "    metrics = soup.find_all('span',class_ = 'data-units-metric')\n",
    "    for index, metr in enumerate(metrics):\n",
    "        if index == 0:\n",
    "            yg['Height']= int(re.sub(' cm', '', metr.text))\n",
    "        if index == 1:\n",
    "            yg['Weight']= int(re.sub(' kg', '', metr.text))\n",
    "            \n",
    "    return yg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_Create_file(path, seas, champ):\n",
    "    \"\"\" Given the season and leagues formatted lists, return an object with all players profile links.\n",
    "    Parameters:\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\n",
    "    \"\"\"   \n",
    "    global all_pages\n",
    "    if os.path.isfile(path):\n",
    "        j_file = open(path)\n",
    "        all_players_pages = json.load(j_file)\n",
    "        j_file.close()\n",
    "    else: \n",
    "        get_allpages(seasons=seas, championship=champ)\n",
    "        for page in tqdm_notebook(all_pages, total=len(all_pages)):\n",
    "            all_players_pages = get_playerlinks(page)\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(list(all_players_pages), f, ensure_ascii=False, indent=4)\n",
    "        f.close()\n",
    "    return all_players_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_list(ALL_team_player_pages):\n",
    "    \"\"\"Given all players profile links, returns final object with info on every player.\n",
    "    Parameters:\n",
    "    - ALL_team_player_pages: every player profile link\n",
    "    \"\"\"  \n",
    "    final_list = []\n",
    "    count = 0\n",
    "    for player_page in tqdm_notebook(ALL_team_player_pages, total=len(ALL_team_player_pages)): \n",
    "        try:\n",
    "            final_list.append(scrape_player('https://www.fifaindex.com' + player_page))\n",
    "        except ValueError:\n",
    "            count += 1\n",
    "            final_list.append({'ERROR': player_page})  \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dataframe(path, seas, champ):\n",
    "    \"\"\"Given the season and leagues formatted lists, returns a Dataframe with all players infos.\n",
    "    Parameters (same as Open_Create_file()):\n",
    "    - path: to check if the wanted object already exists (if it does the function just open the file)\n",
    "    - seas: seasons formatted list\n",
    "    - champ: leagues formatted list\"\"\"\n",
    "    all_players_pages = Open_Create_file(path, seas, champ)\n",
    "    final = get_final_list(all_players_pages)\n",
    "    if len(all_players_pages) == len(final):\n",
    "        print('No Error')\n",
    "    else: \n",
    "        print('!!!Error!!!')\n",
    "    df = pd.DataFrame(final)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section all the previously presented functions are used to obtain a csv file with all teams attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "all_pages = set()\n",
    "player_links = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Seasons and Leagues (TO CUSTOMIZED!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEAGUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major European Leagues \n",
    "leagues = ['league=31', 'league=13','league=16','league=19','league=53'] \n",
    "# Minor Leagues\n",
    "minor_leagues = ['league=14', 'league=17', 'league=20', 'league=32', 'league=54']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ANNUAL DATA \n",
    "years = ['fifa10_6', 'fifa11_7','fifa12_8','fifa13_11','fifa14_12','fifa15_16','fifa16_19','fifa17_75','fifa18_175','fifa19_280','fifa20_358','fifa21_421','fifa22_487'] # If possible always 1st observation in September when the market is closed else August\n",
    "\n",
    "# FOR WEEKLY DATA (season format)\n",
    "weeks = {'fifa10_6', 'fifa11_7', 'fifa12_8', 'fifa12_9', 'fifa13_11', 'fifa14_12', 'fifa15_16'}\n",
    "for i in range(19,60): # FIFA16\n",
    "    weeks.add('fifa16_' + str(i))\n",
    "for j in range(74,144): # FIFA17\n",
    "    weeks.add('fifa17_' + str(j))\n",
    "for k in range(174, 246): # FIFA18\n",
    "    weeks.add('fifa18_' + str(k))\n",
    "for a in range(279, 344): # FIFA19\n",
    "    weeks.add('fifa19_' + str(a))\n",
    "for b in range(354,416): # FIFA20\n",
    "    weeks.add('fifa20_' + str(b))\n",
    "for c in range(420, 465): # FIFA21\n",
    "    weeks.add('fifa21_' + str(c))\n",
    "for d in range(487, 528): # FIFA22\n",
    "    weeks.add('fifa22_' + str(d))\n",
    "\n",
    "# FOR UPDATE: just add new weeks\n",
    "new_weeks = ['fifa15_17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RUN TO GET DATAFRAME!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_dataframe(path = 'proof.json', seas=new_weeks, champ=leagues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"players.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
